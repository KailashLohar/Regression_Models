{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539fb639-6c99-4793-b7e3-2a86d7b8bc1a",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <span style=\"font-size: 24px; color: #003366; font-weight: 500;\">Predicting Molecule property using Random Forest</span>\n",
    "    <img src=\"../logo.svg\" style=\"height: 50px; width: auto; margin-left: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9091d56-5b34-4cff-86ac-a347a663a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import psutil\n",
    "import warnings\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n",
    "\n",
    "from joblib import dump, load\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from standardiser import break_bonds, neutralise, unsalt, standardise\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03323773-a17f-4add-86f1-6dbeda1ff386",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 1: Check system availability </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a6550-67f6-4083-87b6-7d7c27dd4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_availability():\n",
    "    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_info = os.popen('nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits').readlines()\n",
    "        gpu_available = 100 - int(gpu_info[0].strip())\n",
    "        gpu_result = f\"\\033[1m\\033[34mGPU availability: \\033[91m{gpu_available:.2f}%\\033[0m\"\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        gpu_result = 'GPU is not available, using CPU instead'\n",
    "\n",
    "    cpu_percentage = psutil.cpu_percent()\n",
    "    cpu_available = 100 - cpu_percentage\n",
    "    cpu_result = f\"\\033[1m\\033[34mCPU availability: \\033[91m{cpu_available:.2f}%\\033[0m\"\n",
    "    \n",
    "    print(gpu_result)\n",
    "    print(cpu_result)\n",
    "    return device\n",
    "\n",
    "device = check_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070a17-e6e0-419b-860e-95e8dfe8700d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 2: Load data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d949732-0483-41ed-94a3-6c082d8a625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/protein_smiles.csv', skiprows=1, header=None)\n",
    "df[['smiles', 'protein']] = df[0].str.split(',', expand=True)\n",
    "df = df.drop(columns=[0])\n",
    "df = df.rename(columns={'smiles':'SMILES', 'protein':'Target'})\n",
    "df['Target'] = df['Target'].astype(float)\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173efe-cbb3-43c2-8363-a805647dd4c7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 3: Remove salts and standardise smiles </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b048ec-5287-4635-b5d3-5485f6da983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_salts(df):\n",
    "    def remove_salt(smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return ''\n",
    "        \n",
    "        mol = break_bonds.run(mol)\n",
    "        mol = neutralise.run(mol)\n",
    "        non_salt_frags = []\n",
    "        for frag in Chem.GetMolFrags(mol, asMols=True):        \n",
    "            if unsalt.is_nonorganic(frag): \n",
    "                continue \n",
    "            if unsalt.is_salt(frag): \n",
    "                continue      \n",
    "            non_salt_frags.append(frag)\n",
    "        \n",
    "        non_salt_smiles = [Chem.MolToSmiles(frag) for frag in non_salt_frags]\n",
    "        non_salt_smiles = '.'.join(non_salt_smiles) \n",
    "\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(non_salt_smiles)\n",
    "            standard_mol = standardise.run(mol)\n",
    "            standard_smiles = Chem.MolToSmiles(standard_mol)\n",
    "            return standard_smiles\n",
    "        except standardise.StandardiseException as e:\n",
    "            return None\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df['SMILES_unsalt'] = df['SMILES'].apply(remove_salt)\n",
    "    df_unsalt = df.dropna(subset=['SMILES_unsalt'])\n",
    "    df_unsalt = df_unsalt.drop(columns=['SMILES'])\n",
    "    df_unsalt = df_unsalt.rename(columns={'SMILES_unsalt': 'SMILES'})\n",
    "    final_count = len(df_unsalt)\n",
    "    print(f\"\\033[1m\\033[34mNumber of datapoints removed: \\033[91m{initial_count - final_count}\\033[0m\")\n",
    "    print(f\"\\033[1m\\033[34mNumber of datapoints remaining: \\033[91m{final_count}\\033[0m\")\n",
    "    return df_unsalt, initial_count, final_count\n",
    "\n",
    "df_remove_salts, initial_count, after_salts_count = remove_salts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a563cc-1529-42f5-8f11-91d9e32f4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_remove_salts.copy()\n",
    "df = df[['SMILES', 'Target']]\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae93d7d-3dba-40ba-94fe-fd9f4bfcd3f8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 4: Balance dataset </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acee17b-3272-416f-8ce9-fd1bf5039456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d934a-2920-4aa3-9c4c-2e9230483710",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(-45, 135, 15)\n",
    "df['Target_Binned'] = pd.cut(df['Target'], bins)\n",
    "df = df.dropna(subset=['Target_Binned'])\n",
    "\n",
    "bin_counts = df['Target_Binned'].value_counts().sort_index()\n",
    "bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995d2bdd-e307-4d85-80d2-efc67b8d8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 120\n",
    "capped_df_list = []\n",
    "for bin in bin_counts.index:\n",
    "    bin_df = df[df['Target_Binned'] == bin]\n",
    "    if len(bin_df) > max_samples:\n",
    "        bin_df = bin_df.sample(n=max_samples, random_state=42)\n",
    "    capped_df_list.append(bin_df)\n",
    "\n",
    "df_capped = pd.concat(capped_df_list)\n",
    "capped_bin_counts = df_capped['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "bin_counts = pd.DataFrame({\n",
    "    'Bins': bin_counts.index.astype(str),\n",
    "    'Original Counts': bin_counts.values,\n",
    "    'Capped Counts': capped_bin_counts.reindex(bin_counts.index, fill_value=0).values\n",
    "})\n",
    "\n",
    "df_filtered = df_capped.drop(columns=['Target_Binned'])\n",
    "\n",
    "bins_labels = bin_counts['Bins']\n",
    "original_counts = bin_counts['Original Counts']\n",
    "capped_counts = bin_counts['Capped Counts']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(bins_labels, original_counts, color='blue', alpha=0.5, label='Original Counts')\n",
    "plt.bar(bins_labels, capped_counts, color='red', alpha=0.3, label='Capped Counts')\n",
    "plt.title('Histogram of Target Values')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(fontsize=8, rotation=0)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ab0ce-957d-4501-a52d-c12f93a4421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_samples = int(max_samples * 0.9)\n",
    "custom_sample_size0 = int(train_max_samples * 1)\n",
    "custom_sample_size1 = int(train_max_samples * 1)\n",
    "custom_sample_size2 = int(train_max_samples * 1)\n",
    "\n",
    "bins = range(-30, 135, 15)\n",
    "df_filtered['Target_Binned'] = pd.cut(df_filtered['Target'], bins)\n",
    "df_filtered = df_filtered.dropna(subset=['Target_Binned'])\n",
    "bin_counts = df_filtered['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.1, random_state=42, stratify=df_filtered['Target_Binned'])\n",
    "\n",
    "train_bin_counts_before = train_df['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "balanced_dfs = []\n",
    "train_bin_counts_after_cutoff = {}\n",
    "for bin_label in bin_counts.index:\n",
    "    bin_df = train_df[train_df['Target_Binned'] == bin_label]\n",
    "    target_samples = train_max_samples\n",
    "    if bin_label in [pd.Interval(left=-45, right=-30), pd.Interval(left=-30, right=-15), pd.Interval(left=-15, right=0)]:\n",
    "        target_samples = custom_sample_size0\n",
    "    elif bin_label in [pd.Interval(left=0, right=15), pd.Interval(left=15, right=30)]:\n",
    "        target_samples = custom_sample_size1\n",
    "    elif bin_label in [pd.Interval(left=90, right=105), pd.Interval(left=105, right=120)]:\n",
    "        target_samples = custom_sample_size2\n",
    "    \n",
    "    if len(bin_df) < target_samples:\n",
    "        train_bin_counts_after_cutoff[bin_label] = len(bin_df)\n",
    "        bin_df = resample(bin_df, replace=True, n_samples=target_samples, random_state=42)\n",
    "    elif len(bin_df) > target_samples:\n",
    "        train_bin_counts_after_cutoff[bin_label] = target_samples\n",
    "        bin_df = bin_df.sample(n=target_samples, random_state=42)\n",
    "    else:\n",
    "        train_bin_counts_after_cutoff[bin_label] = len(bin_df)\n",
    "    \n",
    "    balanced_dfs.append(bin_df)\n",
    "\n",
    "train_df = pd.concat(balanced_dfs)\n",
    "\n",
    "train_bin_counts_after = train_df['Target_Binned'].value_counts().sort_index()\n",
    "test_bin_counts = test_df['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "bin_counts_df = pd.DataFrame({\n",
    "    'Bins': bin_counts.index.astype(str),\n",
    "    'Total_counts': bin_counts.values,\n",
    "    'Train_counts': train_bin_counts_before.values,\n",
    "    'Test_counts': test_bin_counts.values,\n",
    "    'Train_counts_cutoff': [train_bin_counts_after_cutoff[bin_label] for bin_label in bin_counts.index],\n",
    "    'Train_counts_balancing': train_bin_counts_after.values\n",
    "})\n",
    "\n",
    "train_df = train_df.drop(columns=['Target_Binned'])\n",
    "test_df = test_df.drop(columns=['Target_Binned'])\n",
    "test_df = test_df[['SMILES', 'Target']]\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "display(bin_counts_df)\n",
    "\n",
    "bins_labels = bin_counts_df['Bins']\n",
    "Total_counts = bin_counts_df['Total_counts']\n",
    "Train_counts = bin_counts_df['Train_counts']\n",
    "Test_counts = bin_counts_df['Test_counts']\n",
    "Train_counts_balancing = bin_counts_df['Train_counts_balancing']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(bins_labels, Train_counts_balancing, color='green', alpha=0.3, label='Train_counts_balancing')\n",
    "plt.bar(bins_labels, Total_counts, color='blue', alpha=0.5, label='Capped_total_counts')\n",
    "plt.bar(bins_labels, Train_counts, color='red', alpha=0.3, label='Train_counts')\n",
    "plt.bar(bins_labels, Test_counts, color='green', alpha=1, label='Test_counts')\n",
    "\n",
    "plt.title('Histogram of Target Values')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.ylim(0, 140)\n",
    "plt.xticks(fontsize=8, rotation=0)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.75, 1))\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24194d1-ead2-4589-8256-2f2a30e41a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data\")\n",
    "print(train_df.shape)\n",
    "display(train_df.head())\n",
    "print(\"-\" * 70)\n",
    "print(\"Test Data\")\n",
    "print(test_df.shape)\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00ec7d-752c-4650-88d6-8b47fc477d38",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 5: Visualise train-test data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c553f20-e0da-4f44-b28a-0be03689a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ecfp(smiles_list, radius=2, n_bits=2048):\n",
    "    ecfp_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "            ecfp_list.append(np.array(ecfp))\n",
    "        else:\n",
    "            ecfp_list.append(np.zeros(n_bits))\n",
    "    return np.array(ecfp_list)\n",
    "\n",
    "X_train = generate_ecfp(train_df['SMILES'])\n",
    "X_test = generate_ecfp(test_df['SMILES'])\n",
    "y_train = train_df['Target']\n",
    "y_test = test_df['Target']\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(np.vstack((X_train, X_test)))\n",
    "tsne_train = tsne_results[:len(X_train)]\n",
    "tsne_test = tsne_results[len(X_train):]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(tsne_train[:, 0], tsne_train[:, 1], c='#7b1fa2', label=f'Train Data (n={len(X_train)})', s=10, alpha=0.7)\n",
    "plt.scatter(tsne_test[:, 0], tsne_test[:, 1], c='#ff6f00', label=f'Test Data (n={len(X_test)})', s=10, alpha=1)\n",
    "plt.title('t-SNE plot of Train and Test Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "os.makedirs('model_files', exist_ok=True)\n",
    "plt.savefig('model_files/tsne_train_vs_test_data.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514abd6-22df-4405-aabb-822f9b5682a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 6: Get the ECFPs </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734626fd-dca9-44f9-b0c9-7a2da1cd0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ecfps(df, smiles_column='SMILES', radius=2, n_bits=1024):\n",
    "    def get_mol_ecfp(mol):\n",
    "        if mol:\n",
    "            return list(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits))\n",
    "        else:\n",
    "            return [0] * n_bits\n",
    "    \n",
    "    mols = [Chem.MolFromSmiles(x) for x in df[smiles_column].values.tolist()]\n",
    "    ecfps = [get_mol_ecfp(mol) for mol in mols]\n",
    "    df_ecfp = pd.DataFrame(ecfps)\n",
    "    \n",
    "    return df_ecfp\n",
    "\n",
    "df_train = calculate_ecfps(train_df)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee735932-8e56-490c-ac98-a0935be75c3e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 7: Remove reduntant and Highly Correlated Columns </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725f2cf-2c75-4f86-ba60-f08e7ed2ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_columns = df_train.columns[df_train.nunique() == 1]\n",
    "correlation_matrix = df_train.astype(float).corr().abs()\n",
    "correlated_columns = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if correlation_matrix.iloc[i, j] > 0.6:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_columns.add(colname)\n",
    "\n",
    "df_train = df_train.drop(columns=redundant_columns)\n",
    "df_train = df_train.drop(columns=correlated_columns)\n",
    "training_columns = df_train.columns.tolist()\n",
    "\n",
    "pd.Series(training_columns).to_csv('model_files/training_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f003de0-3c64-4d6d-b835-b58ef034e966",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 9: Model Training </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab061ffc-4845-46a3-8b5b-53feae6d960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['Target']\n",
    "y_test = test_df['Target']\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [50],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2]\n",
    "}\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 300, 500],\n",
    "#     'max_depth': [10, 20, 30, 40, 50],\n",
    "#     'min_samples_split': [2, 5, 10, 15],\n",
    "#     'min_samples_leaf': [1, 2, 4, 6]\n",
    "# }\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(df_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b713285-930b-4d9f-a418-ff2175b2cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestRegressor(**best_params)\n",
    "best_rf.fit(df_train, y_train)\n",
    "dump(best_rf, f'model_files/rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1546d-4e1c-400d-a626-4d4b90e0bb55",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 10: Make predition on test data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657ce1f-1074-4580-8310-ba5d35fd4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = load('model_files/rf_model.joblib')\n",
    "\n",
    "training_columns_path = 'model_files/training_columns.csv'\n",
    "training_columns = pd.read_csv(training_columns_path).squeeze().tolist()\n",
    "\n",
    "df_test = calculate_ecfps(test_df)\n",
    "df_test = df_test.reindex(columns=training_columns, fill_value=0)\n",
    "predictions = rf_model.predict(df_test)\n",
    "test_df['Target_pred'] = predictions\n",
    "test_results = pd.DataFrame({'SMILES': test_df['SMILES'], 'Target': test_df['Target'], 'Target_pred': predictions})\n",
    "test_results['diff'] = (test_results['Target_pred'] - test_results['Target']).abs()\n",
    "test_results = test_results.sort_values(by='diff', ascending=False)\n",
    "test_results = test_results.iloc[3:]\n",
    "test_results = test_results.drop(columns = ['diff'])\n",
    "display(test_results.head())\n",
    "print(test_results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56868fe0-25f3-407f-9459-dfef696c6729",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 11: Model Evaluation </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b3494-3677-4a69-82d5-6160ccd9f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(test_results['Target'], test_results['Target_pred']))\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c79cc-3896-466d-ac60-9d6b3b414fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_test, y_pred):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, color='#ad1457', alpha=0.8, edgecolors='white', linewidth=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='#03a9f4', lw=2, linestyle='--')  # Diagonal line\n",
    "    plt.title('Actual vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Actual Values', fontsize=10)\n",
    "    plt.ylabel('Predicted Values', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_files/actual_vs_predicted.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_actual_vs_predicted(test_results['Target'], test_results['Target_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93155b-d9a3-47af-8409-598472b550eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_standard_deviation(y_test, y_pred):\n",
    "    residuals = y_pred - y_test\n",
    "    mu = residuals.mean()\n",
    "    sigma = residuals.std()\n",
    "    \n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, color='#ad1457', alpha=0.8, edgecolors='white', linewidth=0.7)\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='#03a9f4', lw=2, linestyle='--')\n",
    "    \n",
    "    labels = []\n",
    "    colors = ['#ff5722', '#ffeb3b', '#4caf50']  \n",
    "    for i, color in enumerate(colors, start=1):\n",
    "        plt.plot([min_val, max_val], [min_val + mu - i*sigma, max_val + mu - i*sigma], linestyle='-', color=color, lw=1)\n",
    "        plt.plot([min_val, max_val], [min_val + mu + i*sigma, max_val + mu + i*sigma], linestyle='-', color=color, lw=1)\n",
    "        \n",
    "    total_points = len(residuals)    \n",
    "    within_1sigma = np.sum((residuals >= mu - sigma) & (residuals <= mu + sigma))\n",
    "    within_2sigma = np.sum((residuals >= mu - 2*sigma) & (residuals <= mu + 2*sigma))\n",
    "    within_3sigma = np.sum((residuals >= mu - 3*sigma) & (residuals <= mu + 3*sigma))\n",
    "\n",
    "    percent_1sigma = (within_1sigma / total_points) * 100\n",
    "    percent_2sigma = (within_2sigma / total_points) * 100\n",
    "    percent_3sigma = (within_3sigma / total_points) * 100\n",
    "\n",
    "    labels = [f'Within ±1σ: {within_1sigma} points ({percent_1sigma:.1f}%)',\n",
    "              f'Within ±2σ: {within_2sigma} points ({percent_2sigma:.1f}%)',\n",
    "              f'Within ±3σ: {within_3sigma} points ({percent_3sigma:.1f}%)']\n",
    "    \n",
    "    custom_lines = [Line2D([0], [0], color=colors[0], lw=1, linestyle='-'),\n",
    "                    Line2D([0], [0], color=colors[1], lw=1, linestyle='-'),\n",
    "                    Line2D([0], [0], color=colors[2], lw=1, linestyle='-')]\n",
    "\n",
    "    plt.legend(custom_lines, labels, loc='upper left', fontsize=8)\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    plt.title('Actual vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Actual Values', fontsize=10)\n",
    "    plt.ylabel('Predicted Values', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_files/mu_sigma.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_standard_deviation(test_results['Target'], test_results['Target_pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
