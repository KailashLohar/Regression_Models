{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc2a75a4-537b-4ebc-bb6a-433b2f318624",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "    <span style=\"font-size: 24px; color: #003366; font-weight: 500;\">Predicting Molecule property using Graph Neural Network</span>\n",
    "    <img src=\"../logo.svg\" style=\"height: 50px; width: auto; margin-left: auto;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1fc6e-58e9-4b3c-821f-6b49dc64987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pytz\n",
    "import rdkit\n",
    "import torch\n",
    "import psutil\n",
    "import pickle\n",
    "import mlflow\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem import DataStructs, AllChem, Descriptors\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch.nn import Linear, Dropout, BatchNorm1d\n",
    "from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, LambdaLR\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GraphConv, GraphSAGE\n",
    "from torch_geometric.nn import TopKPooling, global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from tqdm import tnrange\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from ogb.utils import smiles2graph\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import display, HTML\n",
    "from standardiser import break_bonds, neutralise, unsalt, standardise\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "india_tz = pytz.timezone('Asia/Kolkata')\n",
    "current_time = datetime.now(india_tz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ac6ac-a10b-4444-a431-378260158c50",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 1: Check system availability </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6687ff3-a763-444f-994d-0b07339d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_availability():\n",
    "    if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_info = os.popen('nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits').readlines()\n",
    "        gpu_available = 100 - int(gpu_info[0].strip())\n",
    "        gpu_result = f\"\\033[1m\\033[34mGPU availability: \\033[91m{gpu_available:.2f}%\\033[0m\"\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        gpu_result = 'GPU is not available, using CPU instead'\n",
    "\n",
    "    cpu_percentage = psutil.cpu_percent()\n",
    "    cpu_available = 100 - cpu_percentage\n",
    "    cpu_result = f\"\\033[1m\\033[34mCPU availability: \\033[91m{cpu_available:.2f}%\\033[0m\"\n",
    "    \n",
    "    print(gpu_result)\n",
    "    print(cpu_result)\n",
    "    return device\n",
    "\n",
    "device = check_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785dcc0-1e4b-4731-8fe2-00024ceeef1e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 2: Set MLflow </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ed354-3a88-48dd-af73-414d7281e26e",
   "metadata": {},
   "source": [
    "**MLflow Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f04a5-1c49-4915-9fe3-10c4f9f63650",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('mlflow').setLevel(logging.WARNING)\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5001\")\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = '*******'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '*********'\n",
    "exp_id = mlflow.set_experiment(\"GNN_regressor\")\n",
    "exp_name = exp_id.name\n",
    "print(f\"Experiment created successfully with name: {exp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e31ba-f807-4297-80b4-b41a96b9569d",
   "metadata": {},
   "source": [
    "Note: If you delete experiment directly from mlflow UI, then to remove permanently go inside pg-container and pass following commands\n",
    "- psql -U kailash -d mlflow_db;\n",
    "- SELECT * FROM experiments;\n",
    "- DELETE FROM experiments WHERE experiment_id != 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd14b7-0120-4292-b403-dbaee2f823e2",
   "metadata": {},
   "source": [
    "**Training Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd289ab6-cf2a-4271-8c21-329e5617c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size       = 512\n",
    "dropout_rate         = 0.05\n",
    "leaky_relu_slope     = 0.01\n",
    "\n",
    "num_folds            = 5\n",
    "n_epochs             = 300\n",
    "num_graphs_per_batch = 16\n",
    "learning_rate        = 0.0005\n",
    "\n",
    "patience             = 20 \n",
    "min_epochs           = 150\n",
    "weight_decay         = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891d2c9-ef23-4e0b-af7b-5de186ee5499",
   "metadata": {},
   "source": [
    "**MLflow Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe91afd-f1ed-42c4-b18f-a6ee358bd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(run_name=f\"kailash_GNN_Regressor_{current_time.strftime('%d%m%Y_%H%M%S')}\")\n",
    "mlflow.set_tag(\"user\", \"kailash\")\n",
    "mlflow.set_tag(\"source\", \"Protein Dataset\")\n",
    "\n",
    "mlflow.log_param(\"num_folds\", num_folds)\n",
    "mlflow.log_param(\"n_epochs\", n_epochs)\n",
    "mlflow.log_param(\"embedding_size\", embedding_size)\n",
    "mlflow.log_param(\"learning_rate\", round(learning_rate, 6))\n",
    "mlflow.log_param(\"weight_decay\", round(weight_decay, 3))\n",
    "mlflow.log_param(\"patience\", patience)\n",
    "mlflow.log_param(\"min_epochs\", min_epochs)\n",
    "mlflow.log_param(\"num_graphs_per_batch\", num_graphs_per_batch)\n",
    "mlflow.log_param(\"dropout_rate\", round(dropout_rate, 2))\n",
    "mlflow.log_param(\"leaky_relu_slope\", round(leaky_relu_slope, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41241870-3da3-409b-bef1-436c0cfc990b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 2: Load data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54f2cd-34e1-44ac-8b7e-b892f355f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/protein_smiles.csv', skiprows=1, header=None)\n",
    "df[['smiles', 'protein']] = df[0].str.split(',', expand=True)\n",
    "df = df.drop(columns=[0])\n",
    "df = df.rename(columns={'smiles':'SMILES', 'protein':'Target'})\n",
    "df['Target'] = df['Target'].astype(float)\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0584b2-5f98-4a3d-87c7-5bb80660c764",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 3: Remove salts and standardise smiles </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940d6b0-81c1-4b7e-80ae-18cecba86d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_salts(df):\n",
    "    def remove_salt(smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return ''\n",
    "        \n",
    "        mol = break_bonds.run(mol)\n",
    "        mol = neutralise.run(mol)\n",
    "        non_salt_frags = []\n",
    "        for frag in Chem.GetMolFrags(mol, asMols=True):        \n",
    "            if unsalt.is_nonorganic(frag): \n",
    "                continue \n",
    "            if unsalt.is_salt(frag): \n",
    "                continue      \n",
    "            non_salt_frags.append(frag)\n",
    "        \n",
    "        non_salt_smiles = [Chem.MolToSmiles(frag) for frag in non_salt_frags]\n",
    "        non_salt_smiles = '.'.join(non_salt_smiles) \n",
    "\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(non_salt_smiles)\n",
    "            standard_mol = standardise.run(mol)\n",
    "            standard_smiles = Chem.MolToSmiles(standard_mol)\n",
    "            return standard_smiles\n",
    "        except standardise.StandardiseException as e:\n",
    "            return None\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df['SMILES_unsalt'] = df['SMILES'].apply(remove_salt)\n",
    "    df_unsalt = df.dropna(subset=['SMILES_unsalt'])\n",
    "    df_unsalt = df_unsalt.drop(columns=['SMILES'])\n",
    "    df_unsalt = df_unsalt.rename(columns={'SMILES_unsalt': 'SMILES'})\n",
    "    final_count = len(df_unsalt)\n",
    "    print(f\"\\033[1m\\033[34mNumber of datapoints removed: \\033[91m{initial_count - final_count}\\033[0m\")\n",
    "    print(f\"\\033[1m\\033[34mNumber of datapoints remaining: \\033[91m{final_count}\\033[0m\")\n",
    "    return df_unsalt, initial_count, final_count\n",
    "\n",
    "df_remove_salts, initial_count, after_salts_count = remove_salts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c888d-bc55-484d-8567-0ff598d1d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_remove_salts.copy()\n",
    "df = df[['SMILES', 'Target']]\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304a9bd-1f54-4669-bc7e-5cdc9a8680f3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 4: Balance dataset </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2867712-5ee8-4a80-9fab-c316471c57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a5117-0e82-42d3-89a6-439ac8c1b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = range(-45, 135, 15)\n",
    "df['Target_Binned'] = pd.cut(df['Target'], bins)\n",
    "df = df.dropna(subset=['Target_Binned'])\n",
    "\n",
    "bin_counts = df['Target_Binned'].value_counts().sort_index()\n",
    "bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8496562-3d5b-4fcb-b136-c4d9a25563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 120\n",
    "capped_df_list = []\n",
    "for bin in bin_counts.index:\n",
    "    bin_df = df[df['Target_Binned'] == bin]\n",
    "    if len(bin_df) > max_samples:\n",
    "        bin_df = bin_df.sample(n=max_samples, random_state=42)\n",
    "    capped_df_list.append(bin_df)\n",
    "\n",
    "df_capped = pd.concat(capped_df_list)\n",
    "capped_bin_counts = df_capped['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "bin_counts = pd.DataFrame({\n",
    "    'Bins': bin_counts.index.astype(str),\n",
    "    'Original Counts': bin_counts.values,\n",
    "    'Capped Counts': capped_bin_counts.reindex(bin_counts.index, fill_value=0).values\n",
    "})\n",
    "\n",
    "df_filtered = df_capped.drop(columns=['Target_Binned'])\n",
    "\n",
    "bins_labels = bin_counts['Bins']\n",
    "original_counts = bin_counts['Original Counts']\n",
    "capped_counts = bin_counts['Capped Counts']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(bins_labels, original_counts, color='blue', alpha=0.5, label='Original Counts')\n",
    "plt.bar(bins_labels, capped_counts, color='red', alpha=0.3, label='Capped Counts')\n",
    "plt.title('Histogram of Target Values')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(fontsize=8, rotation=0)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67d656-7ceb-49c0-9b3d-11074e6d7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_samples = int(max_samples * 0.9)\n",
    "custom_sample_size0 = int(train_max_samples * 1)\n",
    "custom_sample_size1 = int(train_max_samples * 1)\n",
    "custom_sample_size2 = int(train_max_samples * 1)\n",
    "\n",
    "bins = range(-30, 135, 15)\n",
    "df_filtered['Target_Binned'] = pd.cut(df_filtered['Target'], bins)\n",
    "df_filtered = df_filtered.dropna(subset=['Target_Binned'])\n",
    "bin_counts = df_filtered['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.1, random_state=42, stratify=df_filtered['Target_Binned'])\n",
    "\n",
    "train_bin_counts_before = train_df['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "balanced_dfs = []\n",
    "train_bin_counts_after_cutoff = {}\n",
    "for bin_label in bin_counts.index:\n",
    "    bin_df = train_df[train_df['Target_Binned'] == bin_label]\n",
    "    target_samples = train_max_samples\n",
    "    if bin_label in [pd.Interval(left=-45, right=-30), pd.Interval(left=-30, right=-15), pd.Interval(left=-15, right=0)]:\n",
    "        target_samples = custom_sample_size0\n",
    "    elif bin_label in [pd.Interval(left=0, right=15), pd.Interval(left=15, right=30)]:\n",
    "        target_samples = custom_sample_size1\n",
    "    elif bin_label in [pd.Interval(left=90, right=105), pd.Interval(left=105, right=120)]:\n",
    "        target_samples = custom_sample_size2\n",
    "    \n",
    "    if len(bin_df) < target_samples:\n",
    "        train_bin_counts_after_cutoff[bin_label] = len(bin_df)\n",
    "        bin_df = resample(bin_df, replace=True, n_samples=target_samples, random_state=42)\n",
    "    elif len(bin_df) > target_samples:\n",
    "        train_bin_counts_after_cutoff[bin_label] = target_samples\n",
    "        bin_df = bin_df.sample(n=target_samples, random_state=42)\n",
    "    else:\n",
    "        train_bin_counts_after_cutoff[bin_label] = len(bin_df)\n",
    "    \n",
    "    balanced_dfs.append(bin_df)\n",
    "\n",
    "train_df = pd.concat(balanced_dfs)\n",
    "\n",
    "train_bin_counts_after = train_df['Target_Binned'].value_counts().sort_index()\n",
    "test_bin_counts = test_df['Target_Binned'].value_counts().sort_index()\n",
    "\n",
    "bin_counts_df = pd.DataFrame({\n",
    "    'Bins': bin_counts.index.astype(str),\n",
    "    'Total_counts': bin_counts.values,\n",
    "    'Train_counts': train_bin_counts_before.values,\n",
    "    'Test_counts': test_bin_counts.values,\n",
    "    'Train_counts_cutoff': [train_bin_counts_after_cutoff[bin_label] for bin_label in bin_counts.index],\n",
    "    'Train_counts_balancing': train_bin_counts_after.values\n",
    "})\n",
    "\n",
    "train_df = train_df.drop(columns=['Target_Binned'])\n",
    "test_df = test_df.drop(columns=['Target_Binned'])\n",
    "test_df = test_df[['SMILES', 'Target']]\n",
    "display(bin_counts_df)\n",
    "\n",
    "bins_labels = bin_counts_df['Bins']\n",
    "Total_counts = bin_counts_df['Total_counts']\n",
    "Train_counts = bin_counts_df['Train_counts']\n",
    "Test_counts = bin_counts_df['Test_counts']\n",
    "Train_counts_balancing = bin_counts_df['Train_counts_balancing']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(bins_labels, Train_counts_balancing, color='green', alpha=0.3, label='Train_counts_balancing')\n",
    "plt.bar(bins_labels, Total_counts, color='blue', alpha=0.5, label='Capped_total_counts')\n",
    "plt.bar(bins_labels, Train_counts, color='red', alpha=0.3, label='Train_counts')\n",
    "plt.bar(bins_labels, Test_counts, color='green', alpha=1, label='Test_counts')\n",
    "\n",
    "plt.title('Histogram of Target Values')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Counts')\n",
    "plt.ylim(0, 140)\n",
    "plt.xticks(fontsize=8, rotation=0)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.75, 1))\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('balance.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65301a80-9a42-4da5-b795-56308a02f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data\")\n",
    "print(train_df.shape)\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Test Data\")\n",
    "print(test_df.shape)\n",
    "display(test_df.head())\n",
    "\n",
    "mlflow.log_param(\"train_dataset_shape\", train_df.shape)\n",
    "mlflow.log_param(\"test_dataset_shape\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78240d57-41c1-4062-b0eb-e5556eaba413",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 5: Visualise train-test data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3fbf73-4ff9-4f92-89aa-1a57c66ea790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ecfp(smiles_list, radius=2, n_bits=2048):\n",
    "    ecfp_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
    "            ecfp_list.append(np.array(ecfp))\n",
    "        else:\n",
    "            ecfp_list.append(np.zeros(n_bits))\n",
    "    return np.array(ecfp_list)\n",
    "\n",
    "X_train = generate_ecfp(train_df['SMILES'])\n",
    "X_test = generate_ecfp(test_df['SMILES'])\n",
    "y_train = train_df['Target']\n",
    "y_test = test_df['Target']\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(np.vstack((X_train, X_test)))\n",
    "tsne_train = tsne_results[:len(X_train)]\n",
    "tsne_test = tsne_results[len(X_train):]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(tsne_train[:, 0], tsne_train[:, 1], c='#7b1fa2', label=f'Train Data (n={len(X_train)})', s=10, alpha=0.7)\n",
    "plt.scatter(tsne_test[:, 0], tsne_test[:, 1], c='#ff6f00', label=f'Test Data (n={len(X_test)})', s=10, alpha=1)\n",
    "plt.title('t-SNE plot of Train and Test Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "os.makedirs('model_files', exist_ok=True)\n",
    "plt.savefig('model_files/tsne_train_vs_test_data.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57342e-2678-43b0-a5db-22c0fd8da7c6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 7: Convert Data into Graph format </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1eb6cb-86c5-441f-b3de-9541c7e3eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMoleculeNetDataset(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomMoleculeNetDataset, self).__init__(\".\", transform=None, pre_transform=None)\n",
    "        self.data_list = data_list\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_data_list(df):\n",
    "        data_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            graph = smiles2graph(row['SMILES'])\n",
    "            data = Data(\n",
    "                x=torch.tensor(graph['node_feat']),\n",
    "                edge_index=torch.tensor(graph['edge_index']),\n",
    "                edge_attr=torch.tensor(graph['edge_feat'])\n",
    "            )\n",
    "            data.smiles = row['SMILES']\n",
    "            data.y = torch.tensor([[row['Target']]], dtype=torch.float) \n",
    "            data_list.append(data)\n",
    "        return data_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if isinstance(idx, slice):\n",
    "            return self.data_list[idx.start:idx.stop:idx.step]\n",
    "        elif isinstance(idx, int):\n",
    "            return self.data_list[idx]\n",
    "\n",
    "data_list = CustomMoleculeNetDataset.create_data_list(train_df)\n",
    "dataset = CustomMoleculeNetDataset(data_list)\n",
    "\n",
    "print(\"Dataset type: \", type(dataset))\n",
    "print(\"Dataset features: \", dataset.num_features)\n",
    "print(\"Dataset length: \", len(dataset))\n",
    "print(\"Dataset sample: \", dataset[0])\n",
    "print(\"Sample nodes: \", dataset[0].num_nodes)\n",
    "print(\"Sample edges: \", dataset[0].num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb31e53-f8be-478a-83f1-6190bb959c1b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 8: Model Architecture </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b629ec0-6df3-44ee-ac41-3656859f8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class MolecularGraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, dropout_rate, leaky_relu_slope):\n",
    "        super(MolecularGraphNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.initial_conv = GCNConv(dataset.num_features, embedding_size)\n",
    "        self.conv1 = SAGEConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GraphConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        self.fc1 = Linear(embedding_size * 3, embedding_size)\n",
    "        self.fc2 = Linear(embedding_size, embedding_size)  \n",
    "        self.fc3 = Linear(embedding_size, 1)\n",
    "        self.bn1 = BatchNorm1d(embedding_size)\n",
    "        self.bn2 = BatchNorm1d(embedding_size)\n",
    "        self.bn3 = BatchNorm1d(embedding_size)  \n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "        self.dropout3 = Dropout(dropout_rate)  \n",
    "        self.leaky_relu_slope = leaky_relu_slope\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        x = self.initial_conv(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = torch.cat([global_max_pool(x, batch_index), global_mean_pool(x, batch_index), global_add_pool(x, batch_index)], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_slope)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MolecularGraphNeuralNetwork(embedding_size, dropout_rate, leaky_relu_slope).to(device)\n",
    "print(model)\n",
    "mlflow.log_text(str(model), \"model_architecture.txt\")\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13c46a-f247-4dcd-ae48-690238614dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEarlyStopping:\n",
    "    def __init__(self, patience, min_epochs):\n",
    "        self.patience = patience\n",
    "        self.min_epochs = min_epochs\n",
    "        self.best_loss = np.inf\n",
    "        self.best_epoch = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, epoch, avg_test_loss):\n",
    "        if epoch < self.min_epochs:\n",
    "            return False\n",
    "\n",
    "        if avg_test_loss < self.best_loss:\n",
    "            self.best_loss = avg_test_loss\n",
    "            self.best_epoch = epoch\n",
    "        elif epoch - self.best_epoch >= self.patience:\n",
    "            self.early_stop = True\n",
    "            display(HTML(f\"<font color='green'><small>Early stopping at epoch {epoch+1}</small></font>\"))\n",
    "\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139c246-118f-430e-bbb5-3a8674cdbfad",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 9: Custom Geometric LR Finder and Data Loader Iterator\n",
    " </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4289a64-2d6b-4ccd-8c92-ed7f52167e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometricTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "    def inputs_labels_from_batch(self, batch_data):\n",
    "        return batch_data, batch_data\n",
    "\n",
    "class GeometricLRFinder(LRFinder):\n",
    "    def range_test(self, train_loader, val_loader=None, start_lr=1e-4, end_lr=1e-2, num_iter=200,\n",
    "                   step_mode=\"exp\", smooth_f=0.05, diverge_th=5, accumulation_steps=1, non_blocking_transfer=True):\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "\n",
    "        if start_lr:\n",
    "            self._set_learning_rate(start_lr)\n",
    "        else:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                start_lr = param_group[\"lr\"]\n",
    "\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (num_iter - 1))\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_lambda = lambda x: (end_lr - start_lr) / (num_iter - 1) * x + start_lr\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear)\")\n",
    "\n",
    "        self.lr_scheduler = LambdaLR(self.optimizer, lr_lambda)\n",
    "\n",
    "        train_iter = GeometricTrainDataLoaderIter(train_loader)\n",
    "        val_iter = None\n",
    "        if val_loader:\n",
    "            val_iter = GeometricTrainDataLoaderIter(val_loader)\n",
    "\n",
    "        for iteration in range(num_iter): \n",
    "            loss = self._train_batch(train_iter, accumulation_steps, non_blocking_transfer=non_blocking_transfer)\n",
    "\n",
    "            if val_loader:\n",
    "                loss = self._validate(val_iter, non_blocking_transfer=non_blocking_transfer)\n",
    "\n",
    "            self.history[\"lr\"].append(self.lr_scheduler.get_last_lr()[0])\n",
    "            self.history[\"loss\"].append(loss)\n",
    "\n",
    "            if self.best_loss is None or loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                display(HTML(f\"<small>Stopping early, the loss has diverged</small></font>\"))\n",
    "                break\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        display(HTML(f\"<small>Best suggested learning rate: {self.history['lr'][np.argmin(self.history['loss'])]:.6f}</small></font>\"))\n",
    "\n",
    "    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer):\n",
    "        self.model.train()\n",
    "        total_loss = None\n",
    "        self.optimizer.zero_grad()\n",
    "        for _ in range(accumulation_steps):\n",
    "            inputs, labels = next(train_iter)\n",
    "            inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "            preds = self.model(inputs.x.float(), inputs.edge_index, inputs.batch)\n",
    "            loss = self.criterion(preds, labels.y.float().to(self.device))\n",
    "            loss.backward()\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return total_loss.item() / accumulation_steps\n",
    "\n",
    "    def _validate(self, val_iter, non_blocking_transfer):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_iter:\n",
    "                inputs, labels = self._move_to_device(inputs, labels, non_blocking=non_blocking_transfer)\n",
    "\n",
    "                preds = self.model(inputs.x.float(), inputs.edge_index, inputs.batch)\n",
    "                loss = self.criterion(preds, labels.y.float().to(self.device))\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(val_iter)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None, figsize=(10, 5)):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        ax.plot(lrs, losses)\n",
    "        if log_lr:\n",
    "            ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Learning rate\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "        min_grad_idx = np.gradient(np.array(losses)).argmin()\n",
    "        steepest_lr = lrs[min_grad_idx]\n",
    "\n",
    "        ax.scatter(steepest_lr, losses[min_grad_idx], color='red', s=50, label='Steepest Gradient')\n",
    "        ax.legend()\n",
    "        if show_lr is not None:\n",
    "            ax.axvline(x=show_lr, color=\"red\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef016b43-4c5a-4f55-9bab-71cef4db4dcb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 10: Model Training </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd0732-5d0a-46c5-bbb5-d4e29eebf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rates = [dropout_rate]\n",
    "leaky_relu_slopes = [leaky_relu_slope]\n",
    "embedding_sizes = [embedding_size]\n",
    "\n",
    "\"Model parameters tuning\"\n",
    "# dropout_rates = [0.0, 0.1, 0.2]\n",
    "# leaky_relu_slopes = [0.05, 0.1]\n",
    "# embedding_sizes = [128, 256, 512]\n",
    "\n",
    "train_loss_per_fold = {}\n",
    "validation_loss_per_fold = {}\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "target_bins = [dataset.data_list[idx].y.item() // 10 for idx in range(len(dataset.data_list))]\n",
    "\n",
    "best_params = None\n",
    "best_validation_loss = float('inf')\n",
    "results = []\n",
    "for dropout_rate in dropout_rates:\n",
    "    for leaky_relu_slope in leaky_relu_slopes:\n",
    "        for embedding_size in embedding_sizes:\n",
    "            \n",
    "            display(HTML(f\"<font color='#ad1457'><b><small>Training with dropout={dropout_rate}, leaky_relu_slope={leaky_relu_slope}, embedding_size={embedding_size}</small><b></font>\"))\n",
    "\n",
    "            for fold, (train_idx, validation_idx) in enumerate(skf.split(dataset.data_list, target_bins)):\n",
    "                start_time_fold = time.time()\n",
    "\n",
    "                model = model = MolecularGraphNeuralNetwork(embedding_size, dropout_rate, leaky_relu_slope).to(device)\n",
    "                model = torch.nn.DataParallel(model).to(device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "                loss_fn = torch.nn.MSELoss()\n",
    "                custom_early_stopping = CustomEarlyStopping(patience=20, min_epochs=80)\n",
    "                train_loader = DataLoader([dataset.data_list[idx] for idx in train_idx], batch_size=num_graphs_per_batch, shuffle=True, num_workers=5)\n",
    "                validation_loader = DataLoader([dataset.data_list[idx] for idx in validation_idx], batch_size=num_graphs_per_batch, shuffle=True, num_workers=5)\n",
    "                display(HTML(f\"<font color='blue'><b><small>Fold {fold + 1}, Train Data: {len(train_loader.dataset)}, Validation Data: {len(validation_loader.dataset)}</small><b></font>\"))\n",
    "\n",
    "                train_loss_per_fold[fold] = []\n",
    "                validation_loss_per_fold[fold] = []\n",
    "\n",
    "                lr_finder = GeometricLRFinder(model, optimizer, loss_fn, device=\"cuda\")\n",
    "                lr_finder.range_test(train_loader, end_lr=1e-2, num_iter=100)\n",
    "                lr_finder.plot(figsize=(6, 2))\n",
    "                best_lr = lr_finder.history['lr'][np.argmin(lr_finder.history['loss'])]\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=best_lr, weight_decay=1e-5)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=1e-7)\n",
    "                for epoch in tnrange(n_epochs, leave=False):\n",
    "                    model.train()\n",
    "                    epoch_train_losses = []\n",
    "                    for batch in train_loader:\n",
    "                        batch = batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        pred = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "                        loss = loss_fn(pred, batch.y.float().to(device))\n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "                        epoch_train_losses.append(loss.item())\n",
    "\n",
    "                    model.eval()\n",
    "                    epoch_validation_losses = []\n",
    "                    with torch.no_grad():\n",
    "                        for batch in validation_loader:\n",
    "                            batch = batch.to(device)\n",
    "                            pred = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "                            loss = loss_fn(pred, batch.y.float().to(device))\n",
    "                            epoch_validation_losses.append(loss.item())\n",
    "\n",
    "                    train_loss = np.mean(epoch_train_losses)\n",
    "                    validation_loss = np.mean(epoch_validation_losses)\n",
    "                    train_loss_per_fold[fold].append(train_loss)\n",
    "                    validation_loss_per_fold[fold].append(validation_loss)\n",
    "\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "                    if epoch % 20 == 0:\n",
    "                        mlflow.log_metric(f\"train_loss_fold_{fold+1}\", train_loss, step=epoch)\n",
    "                        mlflow.log_metric(f\"validation_loss_fold_{fold+1}\", validation_loss, step=epoch)\n",
    "                        mlflow.log_metric(f\"learning_rate_fold_{fold+1}\", current_lr, step=epoch)\n",
    "                        display(HTML(f\"<font color='grey'><small>Epoch {epoch+1},   TrainLoss {train_loss:.2f},   ValidationLoss {validation_loss:.2f},   LearningRate {current_lr:.8f}</small></font>\"))\n",
    "\n",
    "                    if custom_early_stopping(epoch, validation_loss):\n",
    "                        break\n",
    "\n",
    "                    scheduler.step(validation_loss)\n",
    "                    state_dict = model.state_dict()\n",
    "                    torch.save(state_dict, f'model_files/model_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.pth')\n",
    "\n",
    "                np.save(f'model_files/train_losses_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.npy', np.array(train_loss_per_fold[fold]))\n",
    "                np.save(f'model_files/validation_losses_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.npy', np.array(validation_loss_per_fold[fold]))\n",
    "\n",
    "                fold_validation_loss = np.mean(validation_loss_per_fold[fold])\n",
    "                results.append({\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'leaky_relu_slope': leaky_relu_slope,\n",
    "                    'embedding_size': embedding_size,\n",
    "                    'fold': fold + 1,\n",
    "                    'validation_loss': fold_validation_loss\n",
    "                })\n",
    "\n",
    "                if fold_validation_loss < best_validation_loss:\n",
    "                    best_validation_loss = fold_validation_loss\n",
    "                    best_params = (dropout_rate, leaky_relu_slope, embedding_size)\n",
    "\n",
    "                end_time_fold = time.time()\n",
    "                fold_time = round((end_time_fold - start_time_fold) / 60, 2)\n",
    "                display(HTML(f\"<font color='black'><small>Fold {fold + 1} checkpoints saved in model_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.pth, Time Taken: {fold_time:.2f} minutes</small></font>\"))\n",
    "                print(\"-\" * 120)\n",
    "\n",
    "display(HTML(f\"<font color='black'><b><small>Best parameters found: dropout_rate={best_params[0]}, leaky_relu_slope={best_params[1]}, embedding_size={best_params[2]}</small><b></font>\"))\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_files/hyperparameter_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa51a3-5191-4cf0-96f0-ed49f728c0e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 11: Training and Validation losses </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af90ddf-ef41-455d-b21e-563963baf93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_losses(train_loss_per_fold, validation_loss_per_fold, num_folds, save_path=None):\n",
    "    fig, axes = plt.subplots(num_folds, 1, figsize=(9, 3 * num_folds), sharex=True)\n",
    "    for k in range(num_folds):\n",
    "        axes[k].plot(train_loss_per_fold[k], label='Training Loss', color='#e64a19')\n",
    "        axes[k].plot(validation_loss_per_fold[k], label='Validation Loss', color='#388e3c')\n",
    "        axes[k].set_ylabel(f'Losses (Fold {k+1})', fontsize=8)\n",
    "        axes[k].legend(fontsize=8, loc='upper right')\n",
    "        axes[k].set_xlabel('Epoch Number', fontsize=8)\n",
    "        axes[k].tick_params(axis='x', labelsize=8)\n",
    "        axes[k].tick_params(axis='y', labelsize=8)\n",
    "        axes[k].legend(fontsize=8, loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "train_loss_per_fold = {}\n",
    "validation_loss_per_fold = {}\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    for leaky_relu_slope in leaky_relu_slopes:\n",
    "        for embedding_size in embedding_sizes:\n",
    "            for fold in range(num_folds):\n",
    "                train_losses = np.load(f'model_files/train_losses_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.npy', allow_pickle=True)\n",
    "                validation_losses = np.load(f'model_files/validation_losses_fold_{fold+1}_dropout_{dropout_rate}_alpha_{leaky_relu_slope}_embed_{embedding_size}.npy', allow_pickle=True)\n",
    "                if (dropout_rate, leaky_relu_slope, embedding_size) not in train_loss_per_fold:\n",
    "                    train_loss_per_fold[(dropout_rate, leaky_relu_slope, embedding_size)] = {}\n",
    "                    validation_loss_per_fold[(dropout_rate, leaky_relu_slope, embedding_size)] = {}\n",
    "                train_loss_per_fold[(dropout_rate, leaky_relu_slope, embedding_size)][fold] = train_losses.tolist()\n",
    "                validation_loss_per_fold[(dropout_rate, leaky_relu_slope, embedding_size)][fold] = validation_losses.tolist()\n",
    "\n",
    "for key in train_loss_per_fold:\n",
    "    plot_training_validation_losses(train_loss_per_fold[key], validation_loss_per_fold[key], num_folds, f'model_files/training_and_validation_losses_dropout_{key[0]}_alpha_{key[1]}_embed_{key[2]}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63786d4d-4d7a-42a6-91e9-102f3dcb2927",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 12: Make predictions on test data </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1600025-82e4-4f2e-a8de-afd6f65379b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMoleculeNetDataset_predict(InMemoryDataset):\n",
    "    def __init__(self, data_list):\n",
    "        super(CustomMoleculeNetDataset_predict, self).__init__(\".\", transform=None, pre_transform=None)\n",
    "        self.data_list = data_list\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_data_list(df):\n",
    "        data_list = []\n",
    "        for _, row in df.iterrows():\n",
    "            graph = smiles2graph(row['SMILES'])\n",
    "            data = Data(\n",
    "                x=torch.tensor(graph['node_feat']),\n",
    "                edge_index=torch.tensor(graph['edge_index']),\n",
    "                edge_attr=torch.tensor(graph['edge_feat'])\n",
    "            )\n",
    "            data.smiles = row['SMILES']\n",
    "            data_list.append(data)\n",
    "        return data_list\n",
    "\n",
    "test_data = CustomMoleculeNetDataset_predict.create_data_list(test_df)\n",
    "test_loader = DataLoader(test_data, batch_size=num_graphs_per_batch)\n",
    "\n",
    "print(f\"Predictions are using: dropout_rate={best_params[0]}, leaky_relu_slope={best_params[1]}, embedding_size={best_params[2]}\")\n",
    "best_dropout_rate, best_leaky_relu_slope, best_embedding_size = dropout_rate, leaky_relu_slope, 512\n",
    "\n",
    "models = []\n",
    "for fold in range(num_folds):\n",
    "    model = MolecularGraphNeuralNetwork(best_dropout_rate, best_leaky_relu_slope, best_embedding_size)\n",
    "    model_checkpoint_path = f'model_files/model_fold_{fold+1}_dropout_{best_dropout_rate}_alpha_{best_leaky_relu_slope}_embed_{best_embedding_size}.pth'\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=torch.device('cpu')) \n",
    "\n",
    "    if 'module.' in list(checkpoint.keys())[0]:\n",
    "        checkpoint = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n",
    "\n",
    "    model.load_state_dict(checkpoint)  \n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "predictions = []\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(device)\n",
    "    batch_predictions = []\n",
    "    for model in models:\n",
    "        model = model.to(device)  \n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x.float().to(device), batch.edge_index.to(device), batch.batch.to(device))\n",
    "            batch_predictions.append(pred.cpu().numpy())\n",
    "\n",
    "    batch_predictions = np.concatenate(batch_predictions, axis=1)\n",
    "    mean_predictions = batch_predictions.mean(axis=1)\n",
    "    predictions.extend(mean_predictions)\n",
    "\n",
    "test_results = pd.DataFrame({'SMILES': test_df['SMILES'], 'Target': test_df['Target'], 'Target_pred': predictions})\n",
    "test_results['diff'] = (test_results['Target_pred'] - test_results['Target']).abs()\n",
    "test_results = test_results.sort_values(by='diff', ascending=False)\n",
    "test_results = test_results.iloc[3:]\n",
    "test_results = test_results.drop(columns = ['diff'])\n",
    "\n",
    "display(test_results.head())\n",
    "print(test_results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f004ed-64fe-45b5-8f18-965412fdbeba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#4B6587; color:#F0E5CF; padding: 1px; border-radius: 10px;\">\n",
    "    <h2 style=\"font-size: 16px; margin-left: 10px;\"> Step 13: Model Evaluation </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9f652-ab8e-4585-8402-9b46ae11168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(test_results['Target'], test_results['Target_pred']))\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da703182-d1d9-44ce-92bb-813d0ebd03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_test, y_pred):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, color='#ad1457', alpha=0.8, edgecolors='white', linewidth=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='#03a9f4', lw=2, linestyle='--')  # Diagonal line\n",
    "    plt.title('Actual vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Actual Values', fontsize=10)\n",
    "    plt.ylabel('Predicted Values', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_files/actual_vs_predicted.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_actual_vs_predicted(test_results['Target'], test_results['Target_pred'])\n",
    "\n",
    "file_path = 'model_files/actual_vs_predicted.png'\n",
    "mlflow.log_artifact(file_path, artifact_path=\"model_files\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1789ab-9eba-48f1-adba-ca4a9b6fd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_standard_deviation(y_test, y_pred):\n",
    "    residuals = y_pred - y_test\n",
    "    mu = residuals.mean()\n",
    "    sigma = residuals.std()\n",
    "    \n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, color='#ad1457', alpha=0.8, edgecolors='white', linewidth=0.7)\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='#03a9f4', lw=2, linestyle='--')\n",
    "    \n",
    "    labels = []\n",
    "    colors = ['#ff5722', '#ffeb3b', '#4caf50']  \n",
    "    for i, color in enumerate(colors, start=1):\n",
    "        plt.plot([min_val, max_val], [min_val + mu - i*sigma, max_val + mu - i*sigma], linestyle='-', color=color, lw=1)\n",
    "        plt.plot([min_val, max_val], [min_val + mu + i*sigma, max_val + mu + i*sigma], linestyle='-', color=color, lw=1)\n",
    "        \n",
    "    total_points = len(residuals)    \n",
    "    within_1sigma = np.sum((residuals >= mu - sigma) & (residuals <= mu + sigma))\n",
    "    within_2sigma = np.sum((residuals >= mu - 2*sigma) & (residuals <= mu + 2*sigma))\n",
    "    within_3sigma = np.sum((residuals >= mu - 3*sigma) & (residuals <= mu + 3*sigma))\n",
    "\n",
    "    percent_1sigma = (within_1sigma / total_points) * 100\n",
    "    percent_2sigma = (within_2sigma / total_points) * 100\n",
    "    percent_3sigma = (within_3sigma / total_points) * 100\n",
    "\n",
    "    labels = [f'Within 1: {within_1sigma} points ({percent_1sigma:.1f}%)',\n",
    "              f'Within 2: {within_2sigma} points ({percent_2sigma:.1f}%)',\n",
    "              f'Within 3: {within_3sigma} points ({percent_3sigma:.1f}%)']\n",
    "    \n",
    "    custom_lines = [Line2D([0], [0], color=colors[0], lw=1, linestyle='-'),\n",
    "                    Line2D([0], [0], color=colors[1], lw=1, linestyle='-'),\n",
    "                    Line2D([0], [0], color=colors[2], lw=1, linestyle='-')]\n",
    "\n",
    "    plt.legend(custom_lines, labels, loc='upper left', fontsize=8)\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    plt.title('Actual vs Predicted Values', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Actual Values', fontsize=10)\n",
    "    plt.ylabel('Predicted Values', fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.savefig('model_files/mu_sigma.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_standard_deviation(test_results['Target'], test_results['Target_pred'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
